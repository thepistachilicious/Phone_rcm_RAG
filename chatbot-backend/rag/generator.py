# rag/generator.py
from llama_index.core.query_engine import RetrieverQueryEngine
from rag.retriever import get_all_retrievers
from db.mongo import get_last_n_user_bot_messages
import google.generativeai as genai
import os
from dotenv import load_dotenv
import asyncio

from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor

from rag.retriever import get_all_retrievers  # nháº­p Ä‘Ãºng theo project báº¡n

from rag.llm_utils import analyze_query_with_gemini  
import google.generativeai as genai


load_dotenv()
gemini_api_key = os.getenv("GEMINI_API_KEY")
genai.configure(api_key=gemini_api_key)




# âœ… HÃ m sinh pháº£n há»“i tá»« Gemini
def generate_gemini_response(query: str, context: str, history_context: str) -> str:
    full_prompt = f"""
    1. DÆ°á»›i Ä‘Ã¢y lÃ  Ä‘oáº¡n há»™i thoáº¡i gáº§n nháº¥t giá»¯a ngÆ°á»i dÃ¹ng vÃ  trá»£ lÃ½:
    {history_context}

    2. Káº¿t há»£p vá»›i ngá»¯ cáº£nh tá»« tÃ i liá»‡u liÃªn quan:
    {context}

    3. CÃ¢u há»i hiá»‡n táº¡i: {query}

    Káº¿t há»£p 3 má»¥c trÃªn vÃ  khÃ´ng táº¡o thÃ´ng tin má»›i, táº¡o cÃ¢u tráº£ lá»i cho ngÆ°á»i dÃ¹ng má»™t cÃ¡ch tá»± nhiÃªn, chÃ­nh xÃ¡c.
    CÃ³ thá»ƒ rÃºt gá»n cÃ¢u tráº£ lá»i Ä‘á»ƒ Ä‘Ãºng trá»ng tÃ¢m vÃ  phÃ¹ há»£p vá»›i cÃ¢u há»i nháº¥t.
    """

    print("ğŸ¤– Gá»­i prompt tá»›i Gemini thÃ´ng qua LLM wrapper...")
    model = genai.GenerativeModel("models/gemini-1.5-flash")
    response = model.generate_content(full_prompt)
    return response.text.strip()

# âœ… HÃ m chÃ­nh gá»i tá»« routes
async def get_llama_response(query: str) -> dict:
    print("ğŸ” Äang truy váº¥n táº¥t cáº£ retrievers...")

    try:
        context_lines = await get_last_n_user_bot_messages(n=3)
        history_context = "\n".join(context_lines)
    except Exception as e:
        print("âš ï¸ KhÃ´ng láº¥y Ä‘Æ°á»£c lá»‹ch sá»­:", e)
        history_context = ""
        
    analysis = analyze_query_with_gemini(query, history_context)
    
    if analysis.get("is_greeting") and analysis.get("answered"):
        return {
            "answer": analysis["answered"] or "ChÃ o báº¡n! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ vá» Ä‘iá»‡n thoáº¡i hoáº·c laptop?",
            "is_retrieved": False
        }

    if analysis.get("is_duplicate") and analysis.get("answered"):
        return {
            "answer": analysis["answered"],
            "is_retrieved": False
        }

    if analysis.get("is_ambiguous")  and analysis.get("new_query"):
        query = analysis["new_query"]

    target_name = analysis.get("retrieval_target", "").strip()



    retriever_tuples = get_all_retrievers()
    best_response = None
    best_retriever_name = None
    best_score = -1

    prioritized = []
    fallback = []
    for retriever_name, retriever in retriever_tuples:
        if target_name and target_name.lower() in retriever_name.lower():
            prioritized.append((retriever_name, retriever))
        else:
            fallback.append((retriever_name, retriever))

    all_retrievers = prioritized + fallback

    for retriever_name, retriever in prioritized:
        print(f"ğŸ” Äang truy váº¥n trong: {retriever_name}")

        query_engine = RetrieverQueryEngine.from_args(
            retriever=retriever,
            node_postprocessors=[
                SimilarityPostprocessor(similarity_cutoff=0.7)
            ],
        )

        try:
            response = query_engine.query(query)
        except Exception as e:
            print(f"âŒ Lá»—i khi truy váº¥n {retriever_name}:", e)
            continue

        if not response.source_nodes:  # <--- kiá»ƒm tra trÆ°á»›c
            print(f"âš ï¸ {retriever_name} khÃ´ng tráº£ vá» káº¿t quáº£.")
            continue

        score = sum([node.score or 0 for node in response.source_nodes])  # an toÃ n rá»“i

        if score > best_score:
            best_response = response
            best_score = score
            best_retriever_name = retriever_name


    if not best_response or best_score == 0:
        return {
            "answer": "KhÃ´ng tÃ¬m tháº¥y ná»™i dung phÃ¹ há»£p.",
            "is_retrieved": False
        }

    context = "\n".join([node.node.get_content() for node in best_response.source_nodes])
    print(f"Sá»­ dá»¥ng retriver trÃªn tÃ i liá»‡u {best_retriever_name}")
    
    try:
        answer = generate_gemini_response(query, context, history_context)
    except Exception as e:
        print("ğŸ”¥ Lá»—i trong generate_gemini_response:", e)
        raise e

    return {
        "answer": answer,
        "retriever": best_retriever_name,
        "is_retrieved": True
    }

# náº¿u liÃªn quan tá»›i cÃ¢u trÃªn vÃ  khÃ´ng cáº§n thÃ´ng tin má»›i thÃ¬ khÃ´ng cáº§n Ä‘Æ°a qua rag
